# ğŸ“Š NLP Support Ticket Auto-Routing System
## Final Project Report

**Project Type:** Production-Ready ML Application  
**Domain:** Natural Language Processing (Text Classification)  
**Status:** âœ… Deployed & Operational  
**Author:** Jonathan  
**Date:** January 2026

---

## ğŸ“‹ Executive Summary

This project delivers a **production-ready NLP system** that automatically routes customer support tickets to the correct department with 67.2% accuracy and 1.28ms inference latency. The system implements a **human-in-the-loop** strategy using confidence thresholding, achieving 78% accuracy on 70% of tickets while routing uncertain cases to human agents.

### Key Achievements
- âœ… **End-to-end ML pipeline** from data exploration to deployment
- âœ… **Production web application** with modern UI deployed on Railway
- âœ… **Real-time predictions** with sub-2ms latency
- âœ… **Balanced training approach** addressing severe class imbalance (21:1 ratio)
- âœ… **Operational risk management** through confidence thresholding

---

## ğŸ¯ Problem Statement

### Business Context
Manual ticket routing in customer support is:
- â±ï¸ **Slow:** Human agents take 30-60 seconds per ticket
- ğŸ’° **Expensive:** Requires dedicated routing staff
- ğŸ² **Inconsistent:** Subject to human error and bias
- ğŸ“ˆ **Unscalable:** Bottleneck during high-volume periods

### Technical Challenge
Build a text classification system that:
1. Routes tickets to 7 departments with high accuracy
2. Handles severe class imbalance (21:1 ratio)
3. Provides sub-second inference latency
4. Manages operational risk through confidence-based routing
5. Deploys as a production-ready web application

---

## ğŸ“Š Dataset Analysis

### Dataset Overview
- **Source:** Multilingual support ticket dataset
- **Language:** English tickets only
- **Total Samples:** 16,338 tickets
- **Features:** Subject + Body text
- **Text Length:** 35-1,189 characters (avg: 403 chars)

### Class Distribution

| Department | Count | Percentage | Imbalance Ratio |
|------------|-------|------------|-----------------|
| **Tech Support** | 7,343 | 44.9% | 1.00x (majority) |
| **Product Support** | 3,073 | 18.8% | 2.39x |
| **Customer Service** | 2,646 | 16.2% | 2.77x |
| **Billing** | 1,595 | 9.8% | 4.60x |
| **Returns** | 820 | 5.0% | 8.95x |
| **Sales** | 513 | 3.1% | 14.31x |
| **HR** | 348 | 2.1% | **21.10x** (minority) |

**Key Challenge:** Severe class imbalance with 21:1 ratio between majority and minority classes.

### Data Preprocessing
1. **Language filtering:** Extracted English tickets from multilingual dataset
2. **Text concatenation:** Combined subject + body for full context
3. **Text cleaning:** Lowercase, whitespace normalization, special character handling
4. **Label mapping:** Consolidated similar queues (e.g., IT Support â†’ Tech Support)

---

## ğŸ§  Methodology

### 1. Exploratory Data Analysis (EDA)
**Notebook:** `01_EDA.ipynb`

**Key Findings:**
- Highly imbalanced dataset requiring special handling
- Variable text lengths requiring padding strategy
- Clear semantic differences between departments
- Some overlap between Customer Service and other categories

### 2. Baseline Models
**Notebook:** `02_baseline.ipynb`

**Approach:**
- **Feature Extraction:** TF-IDF (Term Frequency-Inverse Document Frequency)
- **Models Tested:**
  - Dummy Classifier (sanity check)
  - Multinomial Naive Bayes
  - Logistic Regression (best baseline)
  - Linear SVM

**Best Baseline Results:**
- **Model:** TF-IDF + Logistic Regression
- **Accuracy:** 49.2%
- **Macro F1-Score:** 48.9%
- **Conclusion:** Simple models struggle with class imbalance

### 3. Deep Learning Models
**Notebook:** `03_deep_models.ipynb`  
**Script:** `03_deep_models.py`

#### Architecture: TextCNN
```
Input Text (max 200 tokens)
    â†“
Embedding Layer (vocab_size=6,490, embed_dim=200)
    â†“
Parallel CNN Layers (filters=256, kernels=[3,4,5])
    â†“
Batch Normalization + ReLU + Max Pooling
    â†“
Concatenate Features (768 dims)
    â†“
Dropout (0.3)
    â†“
Fully Connected (768 â†’ 384)
    â†“
Dropout (0.3)
    â†“
Output Layer (384 â†’ 7 classes)
```

#### Training Strategy
- **Vocabulary:** 6,490 tokens (top 20K, min_freq=2)
- **Sequence Length:** 200 tokens (95th percentile)
- **Class Weighting:** Square-root softened weights to address imbalance
- **Optimizer:** Adam (lr=0.001)
- **Batch Size:** 32
- **Epochs:** 30
- **Early Stopping:** Patience of 5 epochs

#### Training Progression
| Epoch | Train Acc | Train F1 | Val Acc | Val F1 |
|-------|-----------|----------|---------|--------|
| 1 | 40.1% | 0.194 | 48.9% | 0.221 |
| 10 | 70.4% | 0.715 | 60.3% | 0.539 |
| 20 | 84.7% | 0.851 | 63.2% | 0.573 |
| **30** | **92.1%** | **0.925** | **67.6%** | **0.627** |

**Observations:**
- Steady improvement throughout training
- Some overfitting (train: 92.1% vs val: 67.6%)
- Validation F1 plateaus around epoch 25-30

---

## ğŸ“ˆ Model Performance

### Test Set Results (Final Model: CNN Balanced)

#### Overall Metrics
- **Accuracy:** 67.2%
- **Macro F1-Score:** 58.2%
- **Inference Latency:** 1.28ms per sample
- **Batch Latency (32):** 19.68ms

#### Per-Department Performance

| Department | Precision | Recall | F1-Score | Support | Performance |
|------------|-----------|--------|----------|---------|-------------|
| **Billing** | 0.91 | 0.75 | 0.82 | High | â­â­â­â­â­ Excellent |
| **Tech Support** | 0.71 | 0.84 | 0.77 | Very High | â­â­â­â­ Strong |
| **Customer Service** | 0.53 | 0.56 | 0.54 | Medium | â­â­â­ Moderate |
| **Product Support** | 0.56 | 0.51 | 0.53 | Medium | â­â­â­ Moderate |
| **HR** | 1.00 | 0.36 | 0.53 | Very Low | âš ï¸ Low Recall |
| **Returns** | 0.70 | 0.38 | 0.49 | Low | âš ï¸ Low Recall |
| **Sales** | 0.65 | 0.27 | 0.39 | Very Low | âš ï¸ Poor |

#### Performance Analysis

**Strong Performers:**
- âœ… **Billing:** Excellent precision (91%) and recall (75%) - clear financial terminology
- âœ… **Tech Support:** Best recall (84%) - largest class with diverse examples

**Moderate Performers:**
- ğŸŸ¡ **Customer Service & Product Support:** Balanced but moderate performance (~53% F1)
- ğŸŸ¡ Likely overlap with other categories causing confusion

**Weak Performers:**
- âš ï¸ **HR, Returns, Sales:** Poor recall (27-38%) due to class imbalance
- âš ï¸ High precision but low recall = model is conservative with these labels
- âš ï¸ Insufficient training examples (348-820 samples)

### Comparison: Baseline vs Deep Learning

| Metric | Baseline (TF-IDF + LR) | Deep Learning (CNN) | Improvement |
|--------|------------------------|---------------------|-------------|
| **Accuracy** | 49.2% | 67.2% | +18.0% |
| **Macro F1** | 48.9% | 58.2% | +9.3% |
| **Latency** | N/A | 1.28ms | âš¡ Fast |

**Conclusion:** Deep learning provides significant improvement, especially with class balancing.

---

## ğŸšï¸ Human-in-the-Loop Strategy

### Confidence Thresholding Analysis

The system routes low-confidence predictions to human agents, balancing automation and accuracy.

| Threshold | Coverage | Reject Rate | Auto Accuracy | Auto F1 | Strategy |
|-----------|----------|-------------|---------------|---------|----------|
| 0.50 | 92.4% | 7.6% | 70.2% | 0.662 | Aggressive automation |
| 0.60 | 83.1% | 16.9% | 73.6% | 0.694 | Moderate automation |
| 0.70 | 74.5% | 25.5% | 76.2% | 0.725 | Balanced |
| **0.75** | **70.0%** | **30.0%** | **78.1%** | **0.753** | **â­ Recommended** |
| 0.80 | 63.7% | 36.3% | 79.4% | 0.761 | Conservative |
| 0.85 | 57.2% | 42.8% | 82.0% | 0.779 | Very conservative |

### Recommended Configuration: Threshold = 0.75

**Business Impact:**
- âœ… **Automates 70%** of tickets (11,436 tickets/day if 100 tickets/hour)
- âœ… **Achieves 78.1% accuracy** on automated tickets
- âœ… **Routes 30%** to humans (4,901 tickets/day)
- âœ… **Reduces routing workload by 70%**

**ROI Calculation (Hypothetical):**
- Manual routing cost: $0.50 per ticket
- Automated routing cost: $0.05 per ticket
- Daily volume: 16,000 tickets
- **Savings:** (16,000 Ã— 0.70 Ã— $0.45) = **$5,040/day** = **$1.84M/year**

---

## ğŸŒ Production Deployment

### Web Application Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              Frontend (HTML/CSS/JS)              â”‚
â”‚  - Text input for ticket                        â”‚
â”‚  - Confidence threshold slider                  â”‚
â”‚  - Real-time prediction display                 â”‚
â”‚  - Probability visualization                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         Backend (Flask + Gunicorn)              â”‚
â”‚  - REST API endpoint: POST /predict             â”‚
â”‚  - Health check: GET /health                    â”‚
â”‚  - Model loading with preload                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚          ML Model (PyTorch CNN)                 â”‚
â”‚  - Vocabulary: 6,490 tokens                     â”‚
â”‚  - Model size: ~50MB                            â”‚
â”‚  - Inference: 1.28ms per prediction             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Technology Stack

| Component | Technology | Version | Purpose |
|-----------|-----------|---------|---------|
| **Backend Framework** | Flask | 3.0.0 | Web server & API |
| **Production Server** | Gunicorn | 21.2.0 | WSGI server |
| **ML Framework** | PyTorch (CPU) | 2.5.1+cpu | Model inference |
| **Numerical Computing** | NumPy | 2.0.2 | Array operations |
| **Frontend** | Vanilla JS | - | Interactive UI |
| **Deployment Platform** | Railway | - | Cloud hosting |
| **Version Control** | Git/GitHub | - | Code management |

### Deployment Configuration

**`requirements.txt`:**
```txt
flask==3.0.0
numpy==2.0.2
gunicorn==21.2.0
--extra-index-url https://download.pytorch.org/whl/cpu
torch==2.5.1+cpu
```

**Key Optimization:** PyTorch CPU-only version (205MB vs 2.5GB GPU version)

**`Procfile` / `nixpacks.toml`:**
```bash
gunicorn app:app --bind 0.0.0.0:$PORT --workers 1 --timeout 120 --preload
```

**Configuration Details:**
- `--workers 1`: Single worker for simplicity and memory efficiency
- `--timeout 120`: 2-minute timeout for model loading
- `--preload`: Load model before forking workers (prevents NoneType errors)

### Deployment Challenges & Solutions

#### Challenge 1: Docker Image Size (5.1 GB > 4 GB limit)
**Problem:** PyTorch with CUDA support is 2.5 GB  
**Solution:** Switched to PyTorch CPU-only (205 MB)  
**Result:** Image size reduced to ~2 GB âœ…

#### Challenge 2: Flask Not Found During Installation
**Problem:** `--index-url` exclusively used PyTorch repo  
**Solution:** Changed to `--extra-index-url` to check both PyPI and PyTorch repo  
**Result:** All dependencies install correctly âœ…

#### Challenge 3: Model Architecture Mismatch
**Problem:** app.py had wrong parameters (embed_dim=150, filters=128)  
**Solution:** Updated to match trained model (embed_dim=200, filters=256)  
**Result:** Model loads successfully âœ…

#### Challenge 4: 'NoneType' Object Not Subscriptable
**Problem:** Gunicorn workers didn't inherit global variables  
**Solution:** Added `--preload` flag and lazy loading check  
**Result:** Model accessible in all workers âœ…

### Production Metrics

- **Image Size:** ~2 GB (under 4 GB limit)
- **Build Time:** 3-5 minutes
- **Cold Start:** ~5 seconds
- **Memory Usage:** ~500 MB
- **Inference Latency:** 1.28ms per prediction
- **Throughput:** ~780 predictions/second (theoretical)

---

## ğŸ“ Project Structure

```
NLP_SupportTicket_Project/
â”‚
â”œâ”€â”€ app.py                          # Flask web application (189 lines)
â”œâ”€â”€ requirements.txt                # Production dependencies
â”œâ”€â”€ Procfile                        # Railway deployment config
â”œâ”€â”€ nixpacks.toml                   # Build configuration
â”œâ”€â”€ README.md                       # Project documentation
â”‚
â”œâ”€â”€ notebooks/                      # Jupyter notebooks for development
â”‚   â”œâ”€â”€ 01_EDA.ipynb               # Exploratory data analysis
â”‚   â”œâ”€â”€ 02_baseline.ipynb          # Baseline model experiments
â”‚   â”œâ”€â”€ 03_deep_models.ipynb       # Deep learning experiments
â”‚   â”œâ”€â”€ 03_deep_models.py          # Training script (453 lines)
â”‚   â””â”€â”€ 04_user_inference.py       # Inference testing
â”‚
â”œâ”€â”€ dataset/                        # Data storage
â”‚   â”œâ”€â”€ raw/                       # Original datasets (5 files)
â”‚   â””â”€â”€ processed/                 # Cleaned datasets
â”‚       â”œâ”€â”€ df_english.csv         # Main dataset (16,338 samples)
â”‚       â””â”€â”€ df_english_cleaned.csv # Alternative cleaned version
â”‚
â”œâ”€â”€ models/                         # Trained models
â”‚   â”œâ”€â”€ cnn_balanced.pt            # Production model (50 MB)
â”‚   â”œâ”€â”€ cnn_simple.pt              # Unbalanced CNN
â”‚   â””â”€â”€ baseline_best.joblib       # Best baseline model
â”‚
â”œâ”€â”€ artifacts/                      # Model artifacts
â”‚   â”œâ”€â”€ vocab_balanced.json        # Vocabulary (6,490 tokens)
â”‚   â”œâ”€â”€ label_map_balanced.json    # Label mappings
â”‚   â””â”€â”€ labels.json                # Label configurations
â”‚
â”œâ”€â”€ reports/                        # Evaluation results
â”‚   â”œâ”€â”€ cnn_balanced_test_metrics.json        # Test performance
â”‚   â”œâ”€â”€ cnn_balanced_history.csv              # Training history (30 epochs)
â”‚   â”œâ”€â”€ cnn_balanced_thresholding_val.csv     # Threshold analysis
â”‚   â”œâ”€â”€ baseline_test_metrics.json            # Baseline results
â”‚   â””â”€â”€ baseline_results_val.csv              # Baseline validation
â”‚
â”œâ”€â”€ templates/                      # HTML templates
â”‚   â””â”€â”€ index.html                 # Web UI (100 lines)
â”‚
â””â”€â”€ static/                         # Frontend assets
    â”œâ”€â”€ css/
    â”‚   â””â”€â”€ style.css              # Styling
    â””â”€â”€ js/
        â””â”€â”€ app.js                 # Frontend logic (127 lines)
```

**Total Files:** 23 core files  
**Total Lines of Code:** ~1,000+ lines (excluding notebooks)  
**Model Size:** 50 MB  
**Dataset Size:** 16,338 samples

---

## ğŸ”¬ Technical Deep Dive

### Model Architecture Details

#### Embedding Layer
- **Vocabulary Size:** 6,490 tokens
- **Embedding Dimension:** 200
- **Padding Index:** 0
- **Unknown Token:** 1

#### Convolutional Layers
- **Number of Filters:** 256 per kernel
- **Kernel Sizes:** [3, 4, 5] (capturing 3-5 word phrases)
- **Activation:** ReLU
- **Batch Normalization:** Applied after each convolution
- **Pooling:** Max pooling over sequence length

#### Fully Connected Layers
- **Hidden Layer:** 768 â†’ 384 (with ReLU + Dropout 0.3)
- **Output Layer:** 384 â†’ 7 (logits for 7 departments)
- **Total Parameters:** ~2.5M parameters

### Training Details

#### Class Weighting Strategy
```python
# Compute class weights
class_counts = [7343, 3073, 2646, 1595, 820, 513, 348]
weights = 1.0 / sqrt(class_counts)
normalized_weights = weights / weights.sum() * len(classes)

# Result:
# Tech Support: 0.37x (downweight majority)
# HR: 5.35x (upweight minority)
```

#### Loss Function
```python
criterion = nn.CrossEntropyLoss(weight=class_weights)
```

#### Optimization
- **Optimizer:** Adam
- **Learning Rate:** 0.001 (default)
- **Weight Decay:** None
- **Gradient Clipping:** None

### Inference Pipeline

```python
def predict(text: str, threshold: float = 0.75):
    # 1. Tokenization
    tokens = text.lower().split()
    
    # 2. Encoding
    ids = [vocab.get(tok, vocab['<UNK>']) for tok in tokens][:200]
    ids = ids + [vocab['<PAD>']] * (200 - len(ids))
    
    # 3. Model Forward Pass
    x = torch.tensor([ids])
    logits = model(x)
    probs = F.softmax(logits, dim=1)[0]
    
    # 4. Prediction
    pred_id = probs.argmax()
    confidence = probs[pred_id]
    
    # 5. Thresholding
    if confidence < threshold:
        action = "ROUTE_TO_HUMAN"
    else:
        action = "AUTO_ROUTE"
    
    return {
        "predicted_label": id2label[pred_id],
        "confidence": float(confidence),
        "action": action,
        "all_probabilities": sorted(probs)
    }
```

---

## ğŸ“Š Results Summary

### Quantitative Results

| Metric | Value | Interpretation |
|--------|-------|----------------|
| **Test Accuracy** | 67.2% | Good for 7-class imbalanced problem |
| **Macro F1-Score** | 58.2% | Moderate across all classes |
| **Inference Latency** | 1.28ms | Excellent for real-time use |
| **Throughput** | ~780 pred/sec | Scales to high volume |
| **Model Size** | 50 MB | Lightweight, easy to deploy |
| **Improvement over Baseline** | +18% accuracy | Significant gain from deep learning |

### Qualitative Results

**Strengths:**
- âœ… Fast inference suitable for production
- âœ… Handles majority classes (Tech Support, Billing) very well
- âœ… Confidence scores enable risk management
- âœ… Clean, deployable codebase
- âœ… Modern web interface

**Limitations:**
- âš ï¸ Struggles with minority classes (HR, Sales, Returns)
- âš ï¸ Some confusion between similar categories
- âš ï¸ 30% of tickets still require human review
- âš ï¸ Limited to English language only

---

## ğŸ¯ Business Impact

### Operational Improvements

**Before (Manual Routing):**
- â±ï¸ 30-60 seconds per ticket
- ğŸ’° $0.50 per ticket (labor cost)
- ğŸ² 70-80% accuracy (human error)
- ğŸ“ˆ Bottleneck during peak hours

**After (AI-Assisted Routing):**
- âš¡ 1.28ms per ticket (automated)
- ğŸ’µ $0.05 per ticket (compute cost)
- ğŸ¯ 78% accuracy on automated tickets
- ğŸ“Š Scales linearly with compute

### Cost-Benefit Analysis

**Assumptions:**
- Daily volume: 16,000 tickets
- Manual cost: $0.50/ticket
- Automated cost: $0.05/ticket
- Automation rate: 70%

**Annual Savings:**
```
Automated tickets: 16,000 Ã— 0.70 Ã— 365 = 4,088,000 tickets/year
Savings per ticket: $0.50 - $0.05 = $0.45
Total savings: 4,088,000 Ã— $0.45 = $1,839,600/year
```

**ROI:** ~$1.84M annually (assuming moderate ticket volume)

### Workforce Impact

- **Routing staff:** Reduced by 70% or reassigned to complex cases
- **Human agents:** Focus on 30% uncertain cases (higher value work)
- **Quality:** More consistent routing decisions
- **Speed:** Instant routing vs 30-60 second delays

---

## ğŸš€ Future Improvements

### Short-term (1-3 months)

1. **Address Class Imbalance**
   - Collect more data for minority classes (HR, Sales, Returns)
   - Implement data augmentation (back-translation, paraphrasing)
   - Try focal loss or other imbalance-aware loss functions

2. **Model Enhancements**
   - Experiment with pre-trained embeddings (Word2Vec, GloVe)
   - Try attention mechanisms for interpretability
   - Ensemble multiple models for better accuracy

3. **Deployment Improvements**
   - Add monitoring and logging (MLflow, Weights & Biases)
   - Implement A/B testing framework
   - Add user feedback collection

### Medium-term (3-6 months)

4. **Advanced Models**
   - Fine-tune transformer models (DistilBERT, RoBERTa)
   - Implement multi-task learning (urgency + department)
   - Add few-shot learning for rare categories

5. **Feature Engineering**
   - Extract metadata features (time of day, ticket length, urgency keywords)
   - Add customer history features
   - Implement ticket priority prediction

6. **Explainability**
   - Add attention visualization
   - Implement LIME/SHAP for predictions
   - Show key phrases influencing decisions

### Long-term (6-12 months)

7. **Multi-language Support**
   - Extend to German, French, Spanish tickets
   - Use multilingual transformers (mBERT, XLM-R)
   - Language-specific fine-tuning

8. **Active Learning**
   - Collect labels for uncertain predictions
   - Retrain model on human-corrected examples
   - Continuous improvement loop

9. **Advanced Features**
   - Auto-suggest responses based on ticket content
   - Predict resolution time
   - Identify duplicate/related tickets

---

## ğŸ“š Lessons Learned

### Technical Lessons

1. **Class Imbalance is Critical**
   - Naive training fails on imbalanced data
   - Class weighting significantly improves minority class performance
   - Square-root softening prevents over-weighting minorities

2. **Simple Models Can Be Competitive**
   - TF-IDF + Logistic Regression achieved 49% accuracy
   - Deep learning improved to 67% (+18%)
   - Trade-off: complexity vs marginal gains

3. **Deployment is Non-Trivial**
   - Docker image size constraints require optimization
   - Gunicorn worker management needs careful configuration
   - CPU-only PyTorch is sufficient for inference

4. **Confidence Thresholding is Powerful**
   - Enables risk management in production
   - Balances automation and accuracy
   - Provides clear business value (70% automation)

### Process Lessons

5. **Iterative Development Works**
   - Start with EDA to understand data
   - Build baseline before deep learning
   - Iterate on model architecture

6. **Documentation is Essential**
   - Clear README helps onboarding
   - Deployment guides save time
   - Code comments prevent confusion

7. **Production-First Mindset**
   - Consider deployment constraints early
   - Optimize for latency and size
   - Build with monitoring in mind

### Business Lessons

8. **Human-in-the-Loop is Pragmatic**
   - 100% automation is unrealistic
   - Confidence thresholding manages risk
   - Humans handle edge cases

9. **ROI Justifies Investment**
   - Clear cost savings ($1.84M/year)
   - Measurable efficiency gains (70% automation)
   - Scalability enables growth

---

## ğŸ“ Skills Demonstrated

### Machine Learning
- âœ… Text classification with deep learning
- âœ… Handling severe class imbalance
- âœ… Model evaluation and selection
- âœ… Hyperparameter tuning
- âœ… Confidence calibration

### Deep Learning
- âœ… CNN architecture for NLP
- âœ… PyTorch implementation
- âœ… Training loop with validation
- âœ… Batch normalization and dropout
- âœ… Class-weighted loss functions

### Software Engineering
- âœ… Flask web application development
- âœ… RESTful API design
- âœ… Frontend development (HTML/CSS/JS)
- âœ… Git version control
- âœ… Code organization and modularity

### MLOps & Deployment
- âœ… Docker containerization
- âœ… Cloud deployment (Railway)
- âœ… Production server configuration (Gunicorn)
- âœ… Dependency management
- âœ… Troubleshooting deployment issues

### Data Science
- âœ… Exploratory data analysis
- âœ… Data preprocessing and cleaning
- âœ… Feature engineering
- âœ… Statistical analysis
- âœ… Visualization and reporting

### Product Thinking
- âœ… Problem definition and scoping
- âœ… Business impact analysis
- âœ… ROI calculation
- âœ… User experience design
- âœ… Risk management strategy

---

## ğŸ“– References & Resources

### Datasets
- Multilingual Support Ticket Dataset (internal/proprietary)

### Libraries & Frameworks
- **PyTorch:** https://pytorch.org/
- **Flask:** https://flask.palletsprojects.com/
- **Scikit-learn:** https://scikit-learn.org/
- **NumPy:** https://numpy.org/

### Deployment
- **Railway:** https://railway.app/
- **Gunicorn:** https://gunicorn.org/

### Techniques
- **TextCNN:** Kim, Y. (2014). Convolutional Neural Networks for Sentence Classification
- **Class Imbalance:** Effective approaches to attention-based neural machine translation
- **Confidence Thresholding:** Guo et al. (2017). On Calibration of Modern Neural Networks

---

## ğŸ† Conclusion

This project successfully delivers a **production-ready NLP system** that automates 70% of support ticket routing with 78% accuracy. The system demonstrates:

1. **Technical Excellence:** End-to-end ML pipeline from data to deployment
2. **Business Value:** $1.84M annual savings potential
3. **Production Quality:** Sub-2ms latency, deployed web application
4. **Risk Management:** Human-in-the-loop strategy for uncertain cases
5. **Scalability:** Handles high-volume ticket routing

### Key Takeaways

âœ… **Deep learning significantly outperforms baselines** (+18% accuracy)  
âœ… **Class imbalance requires specialized techniques** (weighted loss, balanced sampling)  
âœ… **Confidence thresholding enables practical deployment** (70% automation, 78% accuracy)  
âœ… **Deployment optimization is critical** (image size, worker config, error handling)  
âœ… **Production ML requires full-stack skills** (ML, backend, frontend, DevOps)

### Project Success Criteria

| Criterion | Target | Achieved | Status |
|-----------|--------|----------|--------|
| Accuracy | >60% | 67.2% | âœ… Exceeded |
| Latency | <10ms | 1.28ms | âœ… Exceeded |
| Deployment | Production-ready | Deployed on Railway | âœ… Complete |
| Automation | >50% | 70% (at 0.75 threshold) | âœ… Exceeded |
| Code Quality | Clean, documented | Well-structured | âœ… Complete |

---

## ğŸ“ Contact & Repository

**GitHub Repository:** https://github.com/ijonathans/nlp-support-ticket  
**Live Demo:** [Deployed on Railway]  
**Author:** Jonathan  
**Date:** January 2026

---

**Project Status:** âœ… **COMPLETE & DEPLOYED**

*This report documents a complete machine learning project from conception to production deployment, demonstrating end-to-end data science and software engineering capabilities.*
